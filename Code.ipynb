{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "CICSC874_Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# import dataset and processing libraries\n",
        "from random import randint\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# result visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib"
      ],
      "outputs": [],
      "metadata": {
        "id": "rRNKx2xLQr4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and Preparing Data"
      ],
      "metadata": {
        "id": "4pot9uDWTRml"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "# load dataset (test12345)\n",
        "mnist = fetch_openml('mnist_784')"
      ],
      "outputs": [],
      "metadata": {
        "id": "2I-m_h6ZW-EF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "# load dataset\n",
        "X, y = mnist[\"data\"], mnist[\"target\"]\n",
        "\n",
        "# Normalize all image data from [0, 255] to [0, 1]\n",
        "X = X / 255\n",
        "\n",
        "# One hot encoding to create labels for the 10 digits (0,1,2,3,4,5,6,7,8,9)\n",
        "\n",
        "# creates a variables holding shape of the new labels\n",
        "digits = 10 # new cols\n",
        "examples = y.shape[0] # new rows\n",
        "\n",
        "# shape from (70000,) to (1, 70000)\n",
        "y = y.reshape(1, examples)\n",
        "\n",
        "# convert from single column target to one hot encoded array (1, 70000, 10)\n",
        "y_new = np.eye(digits)[y.astype('int32')]\n",
        "# shaped to (10, 70000)\n",
        "y_new = y_new.T.reshape(digits, examples)"
      ],
      "outputs": [],
      "metadata": {
        "id": "7M8IeDJFsltA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "# split, shape and shuffle data\n",
        "# set size for training set (60000)\n",
        "m = 60000\n",
        "\n",
        "# splits inputs into X train and test, using sizes above\n",
        "X_train, X_test = X[:m].T, X[m:].T\n",
        "# splits output into y train and test\n",
        "y_train, y_test = y_new[:,:m], y_new[:,m:]\n",
        "\n",
        "# shuffle training data\n",
        "np.random.seed(42) # reproducibility\n",
        "shuffle_index = np.random.permutation(m)\n",
        "X_train, y_train = X_train[:,shuffle_index], y_train[:,shuffle_index]"
      ],
      "outputs": [],
      "metadata": {
        "id": "9tLCS-tjY99d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Visualization"
      ],
      "metadata": {
        "id": "WMwkhIZ-gz0_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "source": [
        "# sanity check to see if pictures correspond with appropriate label from one-hot encoding\n",
        "# chose random image in train set\n",
        "i = randint(0,m-1)\n",
        "\n",
        "# plot train sample after reshaping back into square image\n",
        "plt.imshow(X_train[:,i].reshape(28,28), cmap=matplotlib.cm.binary)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "# Print corresponding label\n",
        "print(f\"Label: {np.argmax(y_train[:,i],axis=0)}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 2\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "CtA8IgCOcyaI",
        "outputId": "1ef13da4-6d20-4657-e311-97f6e39c1651"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 1"
      ],
      "metadata": {
        "id": "SQCHayOYhOts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Required functions"
      ],
      "metadata": {
        "id": "rCM7YTgbhQMH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "'''\n",
        "takes input image and applies a forward pass through a MLP with 1 hidden layer\n",
        "input: \n",
        "X (all training data), \n",
        "W1,b1,W2,b2 (weights and biases of network)\n",
        "output: \n",
        "Y_pred,\n",
        "tmp (calculated outputs and activations)'''\n",
        "def forward_pass(X, weights):\n",
        "    W1, b1, W2, b2 = weights\n",
        "    # calculating output of first layer, matrix multiplication of weights times input + bias\n",
        "    Z1 = np.matmul(W1,X) + b1\n",
        "    # sigmoid activation function applied to output of first/hidden layer\n",
        "    A1 = sigmoid(Z1)\n",
        "    # calculating output of activation values from hidden layer * weights + bias \n",
        "    Z2 = np.matmul(W2,A1) + b2\n",
        "    # softmax activation function applied to output of output layer\n",
        "    A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)\n",
        "    # output of network\n",
        "    Y_pred = A2\n",
        "    # data to pass to back_propagate\n",
        "    tmp = (Z1,A1,Z2,A2)\n",
        "    \n",
        "    return Y_pred, tmp\n",
        "\n",
        "\n",
        "'''\n",
        "takes Y true and pred, network parameters and the values of forward_pass\n",
        "to apply back propagation training to the weights and biases\n",
        "input:\n",
        "Y, \n",
        "Y_pred, \n",
        "weights (W1, b1, W2, b2), \n",
        "momentum, \n",
        "learning_rate, \n",
        "tmp (Z1,A1,Z2,A2),\n",
        "X\n",
        "output:\n",
        "weights (W1, b1, W2, b2)'''\n",
        "def back_propagate(Y, Y_pred, weights, momentum, learning_rate, tmp, X):\n",
        "    ## calculating back propagation deltas\n",
        "    # unpack weights\n",
        "    W1, b1, W2, b2 = weights\n",
        "    # unpack value calculated during forward pass\n",
        "    Z1,A1,Z2,A2 = tmp\n",
        "    # num_samples\n",
        "    m = X.shape[1]\n",
        "    # error at output\n",
        "    A2 = Y_pred # pred == output of layer 2\n",
        "    dZ2 = A2-Y\n",
        "    # backpropagate error to hidden layer\n",
        "    dW2 = (1./m) * np.matmul(dZ2, A1.T)\n",
        "    db2 = (1./m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "    # multiply ouput error by hidden layer weights to calculate activation of hidden layer\n",
        "    dA1 = np.matmul(W2.T, dZ2)\n",
        "    #error at hidden layer (sigmoid y' = y(y-1))\n",
        "    dZ1 = dA1 * (sigmoid(Z1) * (1 - sigmoid(Z1)))\n",
        "    # backpropagate error to input layer\n",
        "    dW1 = (1./m) * np.matmul(dZ1, X.T)\n",
        "    db1 = (1./m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "    # apply momemtum\n",
        "    dW1 = (momentum * dW1 + (1. - momentum) * dW1)\n",
        "    db1 = (momentum * db1 + (1. - momentum) * db1)\n",
        "    dW2 = (momentum * dW2 + (1. - momentum) * dW2)\n",
        "    db2 = (momentum * db2 + (1. - momentum) * db2)\n",
        "    \n",
        "    # update and return weights\n",
        "    W2 = W2 - learning_rate * dW2\n",
        "    b2 = b2 - learning_rate * db2\n",
        "    W1 = W1 - learning_rate * dW1\n",
        "    b1 = b1 - learning_rate * db1\n",
        "    weights = (W1,b1,W2,b2)\n",
        "    return weights\n",
        "    \n",
        "    \n",
        "'''\n",
        "sigmoid function applies the activation function to the output of neuron\n",
        "input: z (output at neuron)\n",
        "output: activation (output of neuron)'''\n",
        "def sigmoid(z):\n",
        "    activation = 1 / (1 + np.exp(-z))\n",
        "    return activation\n",
        "\n",
        "\n",
        "'''\n",
        "compute loss/error of predictions against ground truth\n",
        "input: Y (ground truth table), Y_hat (prediction table)\n",
        "output: L (loss/error)'''\n",
        "def compute_multiclass_loss(Y, Y_hat):\n",
        "    L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))\n",
        "    m = Y.shape[1]\n",
        "    L = -(1/m) * L_sum\n",
        "    return L"
      ],
      "outputs": [],
      "metadata": {
        "id": "J9McsGUgCj31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build and train networks"
      ],
      "metadata": {
        "id": "FsJ3GcUxhBmG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "\n",
        "# set seed so weights initialize in reproducable manner\n",
        "np.random.seed(42) \n",
        "\n",
        "# network parameters\n",
        "n_x = X_train.shape[0] # num input nodes\n",
        "n_h = 64               # num hidden layer nodes\n",
        "learning_rate = 1      # learning rate\n",
        "epochs = 200   # epochs\n",
        "momentum = 0.5         # momentum\n",
        "\n",
        "# setting random biases and weights for input layer\n",
        "W1 = np.random.randn(n_h, n_x)\n",
        "b1 = np.zeros((n_h, 1))\n",
        "# setting random biases and weights for hidden layer\n",
        "W2 = np.random.randn(digits, n_h)\n",
        "b2 = np.zeros((digits, 1))\n",
        "\n",
        "# loop k epochs to train\n",
        "for i in range(epochs):\n",
        "    \n",
        "    # pass train data into network and get output\n",
        "    # also returns inbetween calculated values for back propagation\n",
        "    weights = (W1, b1, W2, b2) # pack weights for easier passing\n",
        "    y_pred, tmp = forward_pass(X_train, weights)\n",
        "\n",
        "    # compute loss using cross entropy (ground truth, predictions)\n",
        "    loss = compute_multiclass_loss(y_train, y_pred)\n",
        "\n",
        "    # compute updated weights using back propagation\n",
        "    W1,b1,W2,b2 = back_propagate(y_train, y_pred, weights, momentum, learning_rate, tmp, X_train)\n",
        "\n",
        "    # report cost every 100 epochs\n",
        "    if (i % 100 == 0):\n",
        "        print(\"Epoch\", i, \"loss: \", loss)\n",
        "\n",
        "print(\"Final loss:\", loss)\n",
        "print()\n",
        "\n",
        "predictions = np.argmax(y_pred, axis=0)\n",
        "labels = np.argmax(y_train, axis=0)\n",
        "\n",
        "print(\"Train data stats:\")\n",
        "print(confusion_matrix(predictions, labels))\n",
        "print(classification_report(predictions, labels))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss:  9.09999229262521\n",
            "Epoch 100 loss:  0.7266731887531831\n",
            "Final loss: 0.5518701951222055\n",
            "\n",
            "Train data stats:\n",
            "[[5343    1  124   69   28  197  100   56   69   42]\n",
            " [   1 6400   92   41   15   43   37  115  166   15]\n",
            " [  71   70 4777  201   64   82  175  147  164   62]\n",
            " [  56   18  182 4919   18  416   13   49  355  112]\n",
            " [  12   11  153   19 4855  108  107  142  109  399]\n",
            " [ 191   30   48  385   44 3933  133   44  318   99]\n",
            " [ 136   11  193   31  165  153 5257   16   90   32]\n",
            " [  41   26  143   69   67   39   17 5318   59  309]\n",
            " [  57  156  212  320  121  319   60   65 4284  144]\n",
            " [  15   19   34   77  465  131   19  313  237 4735]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.89      0.89      6029\n",
            "           1       0.95      0.92      0.94      6925\n",
            "           2       0.80      0.82      0.81      5813\n",
            "           3       0.80      0.80      0.80      6138\n",
            "           4       0.83      0.82      0.83      5915\n",
            "           5       0.73      0.75      0.74      5225\n",
            "           6       0.89      0.86      0.88      6084\n",
            "           7       0.85      0.87      0.86      6088\n",
            "           8       0.73      0.75      0.74      5738\n",
            "           9       0.80      0.78      0.79      6045\n",
            "\n",
            "    accuracy                           0.83     60000\n",
            "   macro avg       0.83      0.83      0.83     60000\n",
            "weighted avg       0.83      0.83      0.83     60000\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUziYBeoMxGG",
        "outputId": "5f5bd7a0-857d-42cc-c8d0-88b1ea10764f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results for test data"
      ],
      "metadata": {
        "id": "e2BtVnAohGn9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "## 100 epochs\n",
        "# pass test data into trained network and get output\n",
        "weights = W1, b1, W2, b2\n",
        "y_pred,_ = forward_pass(X_test, weights)\n",
        "\n",
        "cost = compute_multiclass_loss(y_test, y_pred)\n",
        "print(\"Final loss:\", cost)\n",
        "\n",
        "predictions = np.argmax(y_pred, axis=0)\n",
        "labels = np.argmax(y_test, axis=0)\n",
        "\n",
        "print(\"Test data stats:\")\n",
        "print(confusion_matrix(predictions, labels))\n",
        "print(classification_report(predictions, labels))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final loss: 0.6927345992527022\n",
            "Test data stats:\n",
            "[[ 867    0   31   13    5   43   32   10   21    9]\n",
            " [   0 1052   31    2    3    4    3   27   11    5]\n",
            " [  21   19  763   30   13   24   52   31   25   11]\n",
            " [   7    4   53  806    2   95    2   19   93   14]\n",
            " [   5    1   26    4  762   23   33   25   29   82]\n",
            " [  40    3   10   78    7  591   27   10   57   27]\n",
            " [  25    2   43    2   35   27  786    3   28    6]\n",
            " [   5    5   28   12   17    7    6  815   16   58]\n",
            " [   9   48   41   52   27   59   12   28  627   13]\n",
            " [   1    1    6   11  111   19    5   60   67  784]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.84      0.86      1031\n",
            "           1       0.93      0.92      0.93      1138\n",
            "           2       0.74      0.77      0.76       989\n",
            "           3       0.80      0.74      0.77      1095\n",
            "           4       0.78      0.77      0.77       990\n",
            "           5       0.66      0.70      0.68       850\n",
            "           6       0.82      0.82      0.82       957\n",
            "           7       0.79      0.84      0.82       969\n",
            "           8       0.64      0.68      0.66       916\n",
            "           9       0.78      0.74      0.76      1065\n",
            "\n",
            "    accuracy                           0.79     10000\n",
            "   macro avg       0.78      0.78      0.78     10000\n",
            "weighted avg       0.79      0.79      0.79     10000\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lxs8UD_Cj2Hy",
        "outputId": "a69ba8f2-1b67-4d3b-9d97-a15f104c6df6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "## 200 epochs\n",
        "# pass test data into trained network and get output\n",
        "weights = W1, b1, W2, b2\n",
        "y_pred,_ = forward_pass(X_test, weights)\n",
        "\n",
        "cost = compute_multiclass_loss(y_test, y_pred)\n",
        "print(\"Final loss:\", cost)\n",
        "\n",
        "predictions = np.argmax(y_pred, axis=0)\n",
        "labels = np.argmax(y_test, axis=0)\n",
        "\n",
        "print(\"Test data stats:\")\n",
        "print(confusion_matrix(predictions, labels))\n",
        "print(classification_report(predictions, labels))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final loss: 0.5264352336787862\n",
            "Test data stats:\n",
            "[[ 901    0   26    8    4   38   27    7   17    6]\n",
            " [   0 1073   16    3    2    1    3   27    7    4]\n",
            " [  14    8  841   29    9   14   24   34   21    7]\n",
            " [   3    4   33  851    1   69    1   12   71    9]\n",
            " [   2    2   22    2  826   21   26   18   19   70]\n",
            " [  32    2    4   61    7  662   25    4   54   22]\n",
            " [  18    3   31    4   21   25  841    2   23    5]\n",
            " [   3    3   23    9   10    5    2  853   18   42]\n",
            " [   7   39   30   35   17   46    6   21  700   16]\n",
            " [   0    1    6    8   85   11    3   50   44  828]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.87      0.89      1034\n",
            "           1       0.95      0.94      0.94      1136\n",
            "           2       0.81      0.84      0.83      1001\n",
            "           3       0.84      0.81      0.82      1054\n",
            "           4       0.84      0.82      0.83      1008\n",
            "           5       0.74      0.76      0.75       873\n",
            "           6       0.88      0.86      0.87       973\n",
            "           7       0.83      0.88      0.85       968\n",
            "           8       0.72      0.76      0.74       917\n",
            "           9       0.82      0.80      0.81      1036\n",
            "\n",
            "    accuracy                           0.84     10000\n",
            "   macro avg       0.84      0.83      0.83     10000\n",
            "weighted avg       0.84      0.84      0.84     10000\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmgRpltgM2ss",
        "outputId": "c6c3c063-2524-4f2c-ccbf-32be1178541a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mini batch, model 1"
      ],
      "metadata": {
        "id": "YJUTpRuT4s1t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "##Build and train network\n",
        "\n",
        "# set seed so weights initialize in reproducable manner\n",
        "np.random.seed(42) \n",
        "\n",
        "# network parameters\n",
        "n_x = X_train.shape[0] # num input nodes\n",
        "n_h = 64               # num hidden layer nodes\n",
        "learning_rate = 1      # learning rate\n",
        "epochs = 20            # epochs\n",
        "momentum = 0.5         # momentum\n",
        "batch_size = 200      #\n",
        "\n",
        "# setting random biases and weights for input layer\n",
        "W1 = np.random.randn(n_h, n_x)\n",
        "b1 = np.zeros((n_h, 1))\n",
        "# setting random biases and weights for hidden layer\n",
        "W2 = np.random.randn(digits, n_h)\n",
        "b2 = np.zeros((digits, 1))\n",
        "y_pred = np.zeros_like(y_train)\n",
        "# loop k epochs to train\n",
        "for i in range(epochs):\n",
        "    \n",
        "    #batch training\n",
        "    for k in range(0, X_train.shape[1], batch_size):\n",
        "      X_mini = X_train[:,k:k+batch_size]\n",
        "      y_mini = y_train[:,k:k+batch_size]\n",
        "      # pass train data into network and get output\n",
        "      # also returns inbetween calculated values for back propagation\n",
        "      weights = (W1, b1, W2, b2) # pack weights for easier passing\n",
        "      y_pred_mini, tmp = forward_pass(X_mini, weights)\n",
        "\n",
        "      # compute loss using cross entropy (ground truth, predictions)\n",
        "      loss = compute_multiclass_loss(y_mini, y_pred_mini)\n",
        "\n",
        "      # compute updated weights using back propagation\n",
        "      W1,b1,W2,b2 = back_propagate(y_mini, y_pred_mini, weights, momentum, learning_rate, tmp, X_mini)\n",
        "      \n",
        "      y_pred[:,k:k+batch_size] = y_pred_mini\n",
        "      #visualize progress\n",
        "      batch_num = int((k+batch_size)/batch_size)\n",
        "      if (batch_num % 20 == 0):\n",
        "        print(f'Batch progress: {batch_num}/{int(m/batch_size)}')\n",
        "\n",
        "    # report cost every 10 epochs\n",
        "    if (i % 10 == 0):\n",
        "        print(\"Epoch\", i, \"loss: \", loss)\n",
        "\n",
        "print(\"Final loss:\", loss)\n",
        "print()\n",
        "\n",
        "predictions = np.argmax(y_pred, axis=0)\n",
        "labels = np.argmax(y_train, axis=0)\n",
        "\n",
        "print(\"Train data stats:\")\n",
        "print(confusion_matrix(predictions, labels))\n",
        "print(classification_report(predictions, labels))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch progress: 20/300\n",
            "Batch progress: 40/300\n",
            "Batch progress: 60/300\n",
            "Batch progress: 80/300\n",
            "Batch progress: 100/300\n",
            "Batch progress: 120/300\n",
            "Batch progress: 140/300\n",
            "Batch progress: 160/300\n",
            "Batch progress: 180/300\n",
            "Batch progress: 200/300\n",
            "Batch progress: 220/300\n",
            "Batch progress: 240/300\n",
            "Batch progress: 260/300\n",
            "Batch progress: 280/300\n",
            "Batch progress: 300/300\n",
            "Epoch 0 loss:  0.5061789116342031\n",
            "Batch progress: 20/300\n",
            "Batch progress: 40/300\n",
            "Batch progress: 60/300\n",
            "Batch progress: 80/300\n",
            "Batch progress: 100/300\n",
            "Batch progress: 120/300\n",
            "Batch progress: 140/300\n",
            "Batch progress: 160/300\n",
            "Batch progress: 180/300\n",
            "Batch progress: 200/300\n",
            "Batch progress: 220/300\n",
            "Batch progress: 240/300\n",
            "Batch progress: 260/300\n",
            "Batch progress: 280/300\n",
            "Batch progress: 300/300\n",
            "Batch progress: 20/300\n",
            "Batch progress: 40/300\n",
            "Batch progress: 60/300\n",
            "Batch progress: 80/300\n",
            "Batch progress: 100/300\n",
            "Batch progress: 120/300\n",
            "Batch progress: 140/300\n",
            "Batch progress: 160/300\n",
            "Batch progress: 180/300\n",
            "Batch progress: 200/300\n",
            "Batch progress: 220/300\n",
            "Batch progress: 240/300\n",
            "Batch progress: 260/300\n",
            "Batch progress: 280/300\n",
            "Batch progress: 300/300\n",
            "Batch progress: 20/300\n",
            "Batch progress: 40/300\n",
            "Batch progress: 60/300\n",
            "Batch progress: 80/300\n",
            "Batch progress: 100/300\n",
            "Batch progress: 120/300\n",
            "Batch progress: 140/300\n",
            "Batch progress: 160/300\n",
            "Batch progress: 180/300\n",
            "Batch progress: 200/300\n",
            "Batch progress: 220/300\n",
            "Batch progress: 240/300\n",
            "Batch progress: 260/300\n",
            "Batch progress: 280/300\n",
            "Batch progress: 300/300\n",
            "Batch progress: 20/300\n",
            "Batch progress: 40/300\n",
            "Batch progress: 60/300\n",
            "Batch progress: 80/300\n",
            "Batch progress: 100/300\n",
            "Batch progress: 120/300\n",
            "Batch progress: 140/300\n",
            "Batch progress: 160/300\n",
            "Batch progress: 180/300\n",
            "Batch progress: 200/300\n",
            "Batch progress: 220/300\n",
            "Batch progress: 240/300\n",
            "Batch progress: 260/300\n",
            "Batch progress: 280/300\n",
            "Batch progress: 300/300\n",
            "Batch progress: 20/300\n",
            "Batch progress: 40/300\n",
            "Batch progress: 60/300\n",
            "Batch progress: 80/300\n",
            "Batch progress: 100/300\n",
            "Batch progress: 120/300\n",
            "Batch progress: 140/300\n",
            "Batch progress: 160/300\n",
            "Batch progress: 180/300\n",
            "Batch progress: 200/300\n",
            "Batch progress: 220/300\n",
            "Batch progress: 240/300\n",
            "Batch progress: 260/300\n",
            "Batch progress: 280/300\n",
            "Batch progress: 300/300\n",
            "Batch progress: 20/300\n",
            "Batch progress: 40/300\n",
            "Batch progress: 60/300\n",
            "Batch progress: 80/300\n",
            "Batch progress: 100/300\n",
            "Batch progress: 120/300\n",
            "Batch progress: 140/300\n",
            "Batch progress: 160/300\n",
            "Batch progress: 180/300\n",
            "Batch progress: 200/300\n",
            "Batch progress: 220/300\n",
            "Batch progress: 240/300\n",
            "Batch progress: 260/300\n",
            "Batch progress: 280/300\n",
            "Batch progress: 300/300\n",
            "Batch progress: 20/300\n",
            "Batch progress: 40/300\n",
            "Batch progress: 60/300\n",
            "Batch progress: 80/300\n",
            "Batch progress: 100/300\n",
            "Batch progress: 120/300\n",
            "Batch progress: 140/300\n",
            "Batch progress: 160/300\n",
            "Batch progress: 180/300\n",
            "Batch progress: 200/300\n",
            "Batch progress: 220/300\n",
            "Batch progress: 240/300\n",
            "Batch progress: 260/300\n",
            "Batch progress: 280/300\n",
            "Batch progress: 300/300\n",
            "Batch progress: 20/300\n",
            "Batch progress: 40/300\n",
            "Batch progress: 60/300\n",
            "Batch progress: 80/300\n",
            "Batch progress: 100/300\n",
            "Batch progress: 120/300\n",
            "Batch progress: 140/300\n",
            "Batch progress: 160/300\n",
            "Batch progress: 180/300\n",
            "Batch progress: 200/300\n",
            "Batch progress: 220/300\n",
            "Batch progress: 240/300\n",
            "Batch progress: 260/300\n",
            "Batch progress: 280/300\n",
            "Batch progress: 300/300\n",
            "Batch progress: 20/300\n",
            "Batch progress: 40/300\n",
            "Batch progress: 60/300\n",
            "Batch progress: 80/300\n",
            "Batch progress: 100/300\n",
            "Batch progress: 120/300\n",
            "Batch progress: 140/300\n",
            "Batch progress: 160/300\n",
            "Batch progress: 180/300\n",
            "Batch progress: 200/300\n",
            "Batch progress: 220/300\n",
            "Batch progress: 240/300\n",
            "Batch progress: 260/300\n",
            "Batch progress: 280/300\n",
            "Batch progress: 300/300\n",
            "Batch progress: 20/300\n",
            "Batch progress: 40/300\n",
            "Batch progress: 60/300\n",
            "Batch progress: 80/300\n",
            "Batch progress: 100/300\n",
            "Batch progress: 120/300\n",
            "Batch progress: 140/300\n",
            "Batch progress: 160/300\n",
            "Batch progress: 180/300\n",
            "Batch progress: 200/300\n",
            "Batch progress: 220/300\n",
            "Batch progress: 240/300\n",
            "Batch progress: 260/300\n",
            "Batch progress: 280/300\n",
            "Batch progress: 300/300\n",
            "Epoch 10 loss:  0.21401116986652746\n",
            "Batch progress: 20/300\n",
            "Batch progress: 40/300\n",
            "Batch progress: 60/300\n",
            "Batch progress: 80/300\n",
            "Batch progress: 100/300\n",
            "Batch progress: 120/300\n",
            "Batch progress: 140/300\n",
            "Batch progress: 160/300\n",
            "Batch progress: 180/300\n",
            "Batch progress: 200/300\n",
            "Batch progress: 220/300\n",
            "Batch progress: 240/300\n",
            "Batch progress: 260/300\n",
            "Batch progress: 280/300\n",
            "Batch progress: 300/300\n",
            "Batch progress: 20/300\n",
            "Batch progress: 40/300\n",
            "Batch progress: 60/300\n",
            "Batch progress: 80/300\n",
            "Batch progress: 100/300\n",
            "Batch progress: 120/300\n",
            "Batch progress: 140/300\n",
            "Batch progress: 160/300\n",
            "Batch progress: 180/300\n",
            "Batch progress: 200/300\n",
            "Batch progress: 220/300\n",
            "Batch progress: 240/300\n",
            "Batch progress: 260/300\n",
            "Batch progress: 280/300\n",
            "Batch progress: 300/300\n",
            "Batch progress: 20/300\n",
            "Batch progress: 40/300\n",
            "Batch progress: 60/300\n",
            "Batch progress: 80/300\n",
            "Batch progress: 100/300\n",
            "Batch progress: 120/300\n",
            "Batch progress: 140/300\n",
            "Batch progress: 160/300\n",
            "Batch progress: 180/300\n",
            "Batch progress: 200/300\n",
            "Batch progress: 220/300\n",
            "Batch progress: 240/300\n",
            "Batch progress: 260/300\n",
            "Batch progress: 280/300\n",
            "Batch progress: 300/300\n",
            "Batch progress: 20/300\n",
            "Batch progress: 40/300\n",
            "Batch progress: 60/300\n",
            "Batch progress: 80/300\n",
            "Batch progress: 100/300\n",
            "Batch progress: 120/300\n",
            "Batch progress: 140/300\n",
            "Batch progress: 160/300\n",
            "Batch progress: 180/300\n",
            "Batch progress: 200/300\n",
            "Batch progress: 220/300\n",
            "Batch progress: 240/300\n",
            "Batch progress: 260/300\n",
            "Batch progress: 280/300\n",
            "Batch progress: 300/300\n",
            "Batch progress: 20/300\n",
            "Batch progress: 40/300\n",
            "Batch progress: 60/300\n",
            "Batch progress: 80/300\n",
            "Batch progress: 100/300\n",
            "Batch progress: 120/300\n",
            "Batch progress: 140/300\n",
            "Batch progress: 160/300\n",
            "Batch progress: 180/300\n",
            "Batch progress: 200/300\n",
            "Batch progress: 220/300\n",
            "Batch progress: 240/300\n",
            "Batch progress: 260/300\n",
            "Batch progress: 280/300\n",
            "Batch progress: 300/300\n",
            "Batch progress: 20/300\n",
            "Batch progress: 40/300\n",
            "Batch progress: 60/300\n",
            "Batch progress: 80/300\n",
            "Batch progress: 100/300\n",
            "Batch progress: 120/300\n",
            "Batch progress: 140/300\n",
            "Batch progress: 160/300\n",
            "Batch progress: 180/300\n",
            "Batch progress: 200/300\n",
            "Batch progress: 220/300\n",
            "Batch progress: 240/300\n",
            "Batch progress: 260/300\n",
            "Batch progress: 280/300\n",
            "Batch progress: 300/300\n",
            "Batch progress: 20/300\n",
            "Batch progress: 40/300\n",
            "Batch progress: 60/300\n",
            "Batch progress: 80/300\n",
            "Batch progress: 100/300\n",
            "Batch progress: 120/300\n",
            "Batch progress: 140/300\n",
            "Batch progress: 160/300\n",
            "Batch progress: 180/300\n",
            "Batch progress: 200/300\n",
            "Batch progress: 220/300\n",
            "Batch progress: 240/300\n",
            "Batch progress: 260/300\n",
            "Batch progress: 280/300\n",
            "Batch progress: 300/300\n",
            "Batch progress: 20/300\n",
            "Batch progress: 40/300\n",
            "Batch progress: 60/300\n",
            "Batch progress: 80/300\n",
            "Batch progress: 100/300\n",
            "Batch progress: 120/300\n",
            "Batch progress: 140/300\n",
            "Batch progress: 160/300\n",
            "Batch progress: 180/300\n",
            "Batch progress: 200/300\n",
            "Batch progress: 220/300\n",
            "Batch progress: 240/300\n",
            "Batch progress: 260/300\n",
            "Batch progress: 280/300\n",
            "Batch progress: 300/300\n",
            "Batch progress: 20/300\n",
            "Batch progress: 40/300\n",
            "Batch progress: 60/300\n",
            "Batch progress: 80/300\n",
            "Batch progress: 100/300\n",
            "Batch progress: 120/300\n",
            "Batch progress: 140/300\n",
            "Batch progress: 160/300\n",
            "Batch progress: 180/300\n",
            "Batch progress: 200/300\n",
            "Batch progress: 220/300\n",
            "Batch progress: 240/300\n",
            "Batch progress: 260/300\n",
            "Batch progress: 280/300\n",
            "Batch progress: 300/300\n",
            "Final loss: 0.1666956036373846\n",
            "\n",
            "Train data stats:\n",
            "[[5797    0   27    9    7   32   30    8   16   21]\n",
            " [   0 6598   28   12    7   10    7   21   41    9]\n",
            " [  12   45 5648   82   26   23   28   48   38   12]\n",
            " [   8   15   51 5754    6  106    2   17   86   53]\n",
            " [   7    9   34    5 5570   16   32   27   16  134]\n",
            " [  21    7   15  125    7 5062   53    7   81   25]\n",
            " [  33    6   30    5   42   54 5736    2   34    4]\n",
            " [   7   20   47   38   20   10    0 6015   12   98]\n",
            " [  33   27   65   66   25   73   28   16 5478   40]\n",
            " [   5   15   13   35  132   35    2  104   49 5553]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.97      0.98      5947\n",
            "           1       0.98      0.98      0.98      6733\n",
            "           2       0.95      0.95      0.95      5962\n",
            "           3       0.94      0.94      0.94      6098\n",
            "           4       0.95      0.95      0.95      5850\n",
            "           5       0.93      0.94      0.94      5403\n",
            "           6       0.97      0.96      0.97      5946\n",
            "           7       0.96      0.96      0.96      6267\n",
            "           8       0.94      0.94      0.94      5851\n",
            "           9       0.93      0.93      0.93      5943\n",
            "\n",
            "    accuracy                           0.95     60000\n",
            "   macro avg       0.95      0.95      0.95     60000\n",
            "weighted avg       0.95      0.95      0.95     60000\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2EUqUzHpchg",
        "outputId": "4e38b4d9-f022-4f2c-d6a9-170d190ed0a5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "# pass test data into trained network and get output\n",
        "weights = W1, b1, W2, b2\n",
        "y_pred,_ = forward_pass(X_test, weights)\n",
        "\n",
        "cost = compute_multiclass_loss(y_test, y_pred)\n",
        "print(\"Final loss:\", cost)\n",
        "\n",
        "predictions = np.argmax(y_pred, axis=0)\n",
        "labels = np.argmax(y_test, axis=0)\n",
        "\n",
        "print(\"Test data stats:\")\n",
        "print(confusion_matrix(predictions, labels))\n",
        "print(classification_report(predictions, labels))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final loss: 0.20033133006317197\n",
            "Test data stats:\n",
            "[[ 963    0    9    1    1    8   10    1   10    6]\n",
            " [   0 1115    2    4    2    1    3   11    3    6]\n",
            " [   1    3  965   13    8    2    4   19    7    2]\n",
            " [   2    2   16  949    1   34    1   14   18    8]\n",
            " [   0    0    7    1  914    4    7    5    9   26]\n",
            " [   4    1    1   21    0  792    8    1   17    8]\n",
            " [   8    5   11    1   16   18  923    0   14    1]\n",
            " [   2    2    6    7    3    4    0  947    4    9]\n",
            " [   0    7   13   10    4   21    2    3  880    5]\n",
            " [   0    0    2    3   33    8    0   27   12  938]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.97      1009\n",
            "           1       0.98      0.97      0.98      1147\n",
            "           2       0.94      0.94      0.94      1024\n",
            "           3       0.94      0.91      0.92      1045\n",
            "           4       0.93      0.94      0.94       973\n",
            "           5       0.89      0.93      0.91       853\n",
            "           6       0.96      0.93      0.94       997\n",
            "           7       0.92      0.96      0.94       984\n",
            "           8       0.90      0.93      0.92       945\n",
            "           9       0.93      0.92      0.92      1023\n",
            "\n",
            "    accuracy                           0.94     10000\n",
            "   macro avg       0.94      0.94      0.94     10000\n",
            "weighted avg       0.94      0.94      0.94     10000\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9VLLd-TjPZc",
        "outputId": "6619afc5-63b0-4fdf-8ca4-28bde253d627"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 2 "
      ],
      "metadata": {
        "id": "1xs2ECTPhJ-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement scikit MLP"
      ],
      "metadata": {
        "id": "gA4tnkmE-J76"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "source": [
        "# Load and normalize data again\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "X = X / 255\n",
        "\n",
        "# training samples\n",
        "m = 60000\n",
        "\n",
        "# train/test split ()\n",
        "X_train, X_test = X[:m,:], X[m:,:]\n",
        "y_train, y_test = y[:m], y[m:]\n",
        "\n",
        "# shuffle training data\n",
        "np.random.seed(42) # reproducibility\n",
        "shuffle_index = np.random.permutation(m)\n",
        "X_train, y_train = X_train[shuffle_index,:], y_train[shuffle_index]"
      ],
      "outputs": [],
      "metadata": {
        "id": "NgjBZxgC_ijn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "source": [
        "##Build and train scikit MLP network\n",
        "\n",
        "# network parameters\n",
        "n_x = X_train.shape[1] # num input nodes\n",
        "n_h = 64               # num hidden layer nodes\n",
        "learning_rate = 1      # learning rate\n",
        "epochs = 50            # epochs\n",
        "momentum = 0.5         # momentum\n",
        "\n",
        "# initialize\n",
        "classifier_neural_MLP = MLPClassifier(hidden_layer_sizes=n_h, max_iter=epochs, momentum=momentum)\n",
        "# train\n",
        "classifier_neural_MLP.fit(X_train, y_train.ravel())\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=64, learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=50,\n",
              "              momentum=0.5, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ],
      "metadata": {
        "id": "x2_LDDIbpchf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e767ab10-0865-4eb4-cfd7-cb9d7e2c0ad4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "source": [
        "y_pred_MLP.ravel()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['7', '2', '1', ..., '4', '5', '6'], dtype='<U1')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2oR_sky1s3H",
        "outputId": "2db46183-136d-4c86-815d-9dc1df587028"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "source": [
        "# predict\n",
        "y_pred_MLP = classifier_neural_MLP.predict(X_train)\n",
        "\n",
        "print(\"Train data stats:\")\n",
        "print(confusion_matrix(y_pred_MLP, y_train))\n",
        "print(classification_report(y_pred_MLP, y_train))\n",
        "\n",
        "# predict test\n",
        "y_pred_MLP = classifier_neural_MLP.predict(X_test)\n",
        "\n",
        "print(\"Test data stats:\")\n",
        "print(confusion_matrix(y_pred_MLP, y_test))\n",
        "print(classification_report(y_pred_MLP, y_test))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data stats:\n",
            "[[5923    0    0    0    0    0    0    0    0    0]\n",
            " [   0 6742    0    0    0    0    0    0    0    0]\n",
            " [   0    0 5958    0    0    0    0    0    0    0]\n",
            " [   0    0    0 6131    0    0    0    0    0    0]\n",
            " [   0    0    0    0 5842    0    0    0    0    0]\n",
            " [   0    0    0    0    0 5421    0    0    0    0]\n",
            " [   0    0    0    0    0    0 5918    0    0    0]\n",
            " [   0    0    0    0    0    0    0 6265    0    0]\n",
            " [   0    0    0    0    0    0    0    0 5851    0]\n",
            " [   0    0    0    0    0    0    0    0    0 5949]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      5923\n",
            "           1       1.00      1.00      1.00      6742\n",
            "           2       1.00      1.00      1.00      5958\n",
            "           3       1.00      1.00      1.00      6131\n",
            "           4       1.00      1.00      1.00      5842\n",
            "           5       1.00      1.00      1.00      5421\n",
            "           6       1.00      1.00      1.00      5918\n",
            "           7       1.00      1.00      1.00      6265\n",
            "           8       1.00      1.00      1.00      5851\n",
            "           9       1.00      1.00      1.00      5949\n",
            "\n",
            "    accuracy                           1.00     60000\n",
            "   macro avg       1.00      1.00      1.00     60000\n",
            "weighted avg       1.00      1.00      1.00     60000\n",
            "\n",
            "Test data stats:\n",
            "[[ 968    0    6    0    1    3    6    1    6    1]\n",
            " [   1 1122    3    0    0    0    2    6    0    2]\n",
            " [   1    4 1001    8    4    0    4   10    4    0]\n",
            " [   0    2    5  978    1   11    1    3    4    3]\n",
            " [   0    0    4    1  961    0    6    1    5    7]\n",
            " [   1    1    1    8    0  863    5    0    1    4]\n",
            " [   4    2    1    0    5    4  933    0    2    1]\n",
            " [   1    1    4    2    0    1    0  995    5    4]\n",
            " [   3    3    7    4    1    7    1    6  941    5]\n",
            " [   1    0    0    9    9    3    0    6    6  982]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.98       992\n",
            "           1       0.99      0.99      0.99      1136\n",
            "           2       0.97      0.97      0.97      1036\n",
            "           3       0.97      0.97      0.97      1008\n",
            "           4       0.98      0.98      0.98       985\n",
            "           5       0.97      0.98      0.97       884\n",
            "           6       0.97      0.98      0.98       952\n",
            "           7       0.97      0.98      0.98      1013\n",
            "           8       0.97      0.96      0.96       978\n",
            "           9       0.97      0.97      0.97      1016\n",
            "\n",
            "    accuracy                           0.97     10000\n",
            "   macro avg       0.97      0.97      0.97     10000\n",
            "weighted avg       0.97      0.97      0.97     10000\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gguBSXzW07ea",
        "outputId": "6aac8a9e-adc8-4ddf-f7b1-ecd845d62199"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP random search"
      ],
      "metadata": {
        "id": "U3_sk2myhbg8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from sklearn.model_selection import KFold, GridSearchCV, cross_val_predict\n",
        "\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes':[32,64,96,128],#[32,50,80,100,120,150],\n",
        "    'activation':['identity','logistic','tanh','relu'],\n",
        "    'learning_rate':['constant', 'invscaling', 'adaptive'],\n",
        "    'solver':['ibfgs','sgd','adam'],\n",
        "    'batch_size':[100,200,300,500],\n",
        "    'max_iter':[50,100,200,250]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    MLPClassifier(), param_grid=param_grid, cv=7, refit = True, verbose=3, n_jobs=-1,\n",
        "    scoring='accuracy')\n",
        "\n",
        "\n",
        "grid_search.fit(X,y)\n",
        "\n",
        "print('best score {}'.format(grid_search.best_score_)) \n",
        "print('best score {}'.format(grid_search.best_params_))"
      ],
      "outputs": [],
      "metadata": {
        "id": "XnKneuVpXFKQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "source": [
        "##Build and train scikit MLP network with best parameters set\n",
        "best_params = {'solver': 'adam', 'learning_rate': 'constant', 'hidden_layer_sizes': 128, 'batch_size': 250, 'activation': 'logistic'}\n",
        "# initialize\n",
        "classifier_neural_MLP = MLPClassifier(**best_params)\n",
        "# train\n",
        "classifier_neural_MLP.fit(X_train, y_train)\n",
        "# predict\n",
        "y_pred_MLP = classifier_neural_MLP.predict(X_test)"
      ],
      "outputs": [],
      "metadata": {
        "id": "EyyfircgVuq8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "source": [
        "print(\"Test data stats:\")\n",
        "print(confusion_matrix(y_pred_MLP, y_test))\n",
        "print(classification_report(y_pred_MLP, y_test))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test data stats:\n",
            "[[ 972    1    3    0    1    3    5    1    5    1]\n",
            " [   0 1122    0    0    0    1    2    2    0    2]\n",
            " [   1    4 1009    8    0    0    1   10    2    0]\n",
            " [   1    0    5  984    1    8    1    2    4    4]\n",
            " [   2    0    1    0  961    3    3    1    6   11]\n",
            " [   0    1    0    5    0  867    5    1    4    2]\n",
            " [   2    2    4    0    6    5  940    0    5    1]\n",
            " [   1    1    5    5    0    0    0 1005    5    3]\n",
            " [   1    4    5    3    2    3    1    2  941    5]\n",
            " [   0    0    0    5   11    2    0    4    2  980]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.98      0.99       992\n",
            "           1       0.99      0.99      0.99      1129\n",
            "           2       0.98      0.97      0.98      1035\n",
            "           3       0.97      0.97      0.97      1010\n",
            "           4       0.98      0.97      0.98       988\n",
            "           5       0.97      0.98      0.98       885\n",
            "           6       0.98      0.97      0.98       965\n",
            "           7       0.98      0.98      0.98      1025\n",
            "           8       0.97      0.97      0.97       967\n",
            "           9       0.97      0.98      0.97      1004\n",
            "\n",
            "    accuracy                           0.98     10000\n",
            "   macro avg       0.98      0.98      0.98     10000\n",
            "weighted avg       0.98      0.98      0.98     10000\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "id": "FI2LVqsICDgn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2e7ba28-69f0-46ba-e45e-4cb895dc8d84"
      }
    }
  ]
}